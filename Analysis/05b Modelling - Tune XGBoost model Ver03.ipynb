{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a XGBoost binary classifier\n",
    "\n",
    "Given XGBoost's popularity and success on Kaggle I wanted to try it out. This workbook contains 2 sections:\n",
    "\n",
    "* Create the datasets needed to train, validate, and test the model.\n",
    "        Import the features and labels files, combine and split them into 80:20 train:test sets\n",
    "* Tune some hyper-parameters to check if the model can be improved.\n",
    "        Use a random search cross validation to detemine if we can improve on the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 75 # default for me was 75\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, GridSearchCV #cross_validate\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import SCORERS, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "#from sklearn.model_selection import validation_curve\n",
    "\n",
    "#conda install -c conda-forge shap\n",
    "import shap\n",
    "#Load JS visualisation code to Notebook\n",
    "shap.initjs()\n",
    "\n",
    "seed = 207\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to evaluate\n",
    "labelToEval = 'cityScore'\n",
    "\n",
    "# As I am generating all the data I need to extract a decent set of data for eventual testing.\n",
    "featuresDataFilename = 'ModelInput/features.csv'\n",
    "labelsDataFilename = 'ModelInput/labels.csv'\n",
    "# All feature and label data I have available\n",
    "featuresDf = pd.read_csv(featuresDataFilename)\n",
    "print('Features: {}'.format(featuresDf.shape))\n",
    "labelsDf = pd.read_csv(labelsDataFilename)[['cityId', labelToEval]]\n",
    "print('  Labels: {}'.format(labelsDf.shape))\n",
    "\n",
    "# Merge the features and labels so I can extract the various sets\n",
    "dataDf = pd.merge(featuresDf, labelsDf, on='cityId', how='inner')\n",
    "print('Combined: {}'.format(dataDf.shape))\n",
    "\n",
    "# Start at 2nd column, i.e. exclude cityId\n",
    "allX = pd.get_dummies(dataDf.iloc[:,2:len(featuresDf.columns)])\n",
    "print('\\n   All X: {}'.format(allX.shape))\n",
    "allY = dataDf[labelToEval]\n",
    "print('   All y: {}'.format(allY.shape))\n",
    "\n",
    "# Create train and test split values. \n",
    "# train will be split further using the StratifiedKFold random search below for parameter Opt...\n",
    "trainXouter, testX, trainYouter, testY = train_test_split(allX, allY, test_size=0.20, random_state=seed)\n",
    "\n",
    "print('\\n  testX: {}'.format(testX.shape))\n",
    "print('  testY: {}'.format(testY.shape))\n",
    "print('\\n trainXouter: {}'.format(trainXouter.shape))\n",
    "print(' trainYouter: {}'.format(trainYouter.shape))\n",
    "\n",
    "# Now create the DMatrix files that XGBoost prefers\n",
    "#testDM = xgb.DMatrix(testX, testY)\n",
    "#trainOuterDM = xgb.DMatrix(trainXouter, trainYouter)\n",
    "#allDM = xgb.DMatrix(allX, allY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I only have the top quantile at \"good\" I need to adjust the post weights!\n",
    "# using ratio of 1 creates poor models for me...\n",
    "ratio = float(trainYouter.groupby(trainYouter).count()[0] / trainYouter.groupby(trainYouter).count()[1])\n",
    "print('Est ratio: {:.3f}'.format(ratio))\n",
    "\n",
    "# As we are using top 25% this ratio for the \"real\" situation is 3, hardcoding this\n",
    "ratio = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a random search to see if we can improve the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=ratio) #, seed=seed)\n",
    "param_dist = {\n",
    "    'n_estimators': stats.randint(10, 300),\n",
    "    'learning_rate': [0.05],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'min_child_weight': [1, 2, 3, 4],\n",
    "    'subsample': stats.uniform(0.0, 0.3),          # range is 0.10 - 0.30\n",
    "    'colsample_bytree': stats.uniform(0.7, 0.3),   # range is 0.60 - 0.35\n",
    "#    'colsample_bylevel': stats.uniform(0.1, 0.85),  # range is 0.60 - 0.35\n",
    "}\n",
    "clf = RandomizedSearchCV(clf_xgb, param_distributions = param_dist, n_iter = 20,\n",
    "                         scoring='balanced_accuracy', error_score = 0, verbose = 3, n_jobs = 2,\n",
    "                         cv=10, iid=False, refit='balanced_accuracy') #, random_state=seed)\n",
    "\n",
    "# scoring='average_precision' which is meant to be the 'aucpr' equivalent\n",
    "# Using it gives great recall, but also horrible precision!\n",
    "# neg_log_loss, balanced_accuracy, f1, neg_log_loss\n",
    "numFolds = 10\n",
    "skf = StratifiedKFold(n_splits=numFolds) #, random_state=seed)\n",
    "\n",
    "results = []\n",
    "counter = 1\n",
    "for train_index, val_index in skf.split(trainXouter, trainYouter):\n",
    "    print('\\nEntering loop number {} of {}\\n'.format(counter, numFolds))\n",
    "    counter += 1\n",
    "    X_train, X_val = trainXouter.iloc[train_index], trainXouter.iloc[val_index]\n",
    "    y_train, y_val = trainYouter.iloc[train_index], trainYouter.iloc[val_index]\n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set = [(X_train, y_train), (X_val, y_val)],\n",
    "            eval_metric= 'auc',\n",
    "            verbose=30,\n",
    "            early_stopping_rounds=30,\n",
    "           )\n",
    "\n",
    "    # 'eval_metric': ['logloss', 'error', 'auc', 'aucpr'],\n",
    "\n",
    "    # Create the evaluation metrics for choosing a model\n",
    "    estimator = clf.best_estimator_\n",
    "    y_pred = estimator.predict(testX)\n",
    "    recall=recall_score(testY, y_pred)\n",
    "    ct = pd.crosstab(\n",
    "        pd.Series(testY.values, name='Actual'),\n",
    "        pd.Series(y_pred, name='Predicted'),\n",
    "        margins=True\n",
    "    )\n",
    "    falsePositives = ct.iloc[0,1]\n",
    "    falseNegatives = ct.iloc[1,0]\n",
    "    total = len(testX)\n",
    "    misclassification = round((falseNegatives + falsePositives)/total, 4)\n",
    "    \n",
    "    # Add to results\n",
    "    result = {\n",
    "        'estNumber': (counter - 2),\n",
    "        'bestEstimator': estimator,\n",
    "        'recall': recall,\n",
    "        'misclassification': misclassification,\n",
    "        'confusionMatrix': ct\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n\\nAll done!\")\n",
    "\n",
    "# Fitting (cv=5) folds for each of (n_iter=60) candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.sort_values(['recall', 'misclassification'], ascending=[False, True], inplace=True)\n",
    "print(df[['estNumber', 'recall', 'misclassification']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for a combination of high recall and low misclassification. Look at the top results above and choose 2 for further checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chosenEstNumber1 = 4\n",
    "chosenEstNumber2 = 9\n",
    "\n",
    "cm1 = df[df['estNumber'] == chosenEstNumber1].confusionMatrix.iloc[0]\n",
    "cm2 = df[df['estNumber'] == chosenEstNumber2].confusionMatrix.iloc[0]\n",
    "\n",
    "'recall', 'misclassification'\n",
    "\n",
    "print('\\nEstimator {}:'.format(chosenEstNumber1))\n",
    "print('')\n",
    "print('           Recall: {:.3f}'.format(df[df['estNumber'] == chosenEstNumber1].recall.iloc[0]))\n",
    "print('Misclassification: {:.3f}'.format(df[df['estNumber'] == chosenEstNumber1].misclassification.iloc[0]))\n",
    "print('')\n",
    "print(cm1)\n",
    "print('\\nEstimator {}:'.format(chosenEstNumber2))\n",
    "print('')\n",
    "print('           Recall: {:.3f}'.format(df[df['estNumber'] == chosenEstNumber2].recall.iloc[0]))\n",
    "print('Misclassification: {:.3f}'.format(df[df['estNumber'] == chosenEstNumber2].misclassification.iloc[0]))\n",
    "print('')\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one estimator (model), print out the key parameters and persist to disk using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenEstNumber = 4\n",
    "clf = df[df['estNumber'] == chosenEstNumber].bestEstimator.iloc[0]\n",
    "for param in clf.get_params():\n",
    "    print('{:>20}: {:>10}'.format(param, str(clf.get_params()[param])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{:%Y%m%d_%H%M%S}.xgbmodel'.format(datetime.now())\n",
    "pickle.dump(clf,\n",
    "            open('./Models/{}'.format(filename),\n",
    "                 'wb'))\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
